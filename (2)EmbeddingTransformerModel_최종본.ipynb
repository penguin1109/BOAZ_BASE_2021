{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "(2)EmbeddingTransformerModel_최종본.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Dmpz-zo2kB0Q"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUOWRUTE0w4-",
        "outputId": "59d02d87-ede6-4b07-baba-70085772fe2e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kfoqhjDSC7_"
      },
      "source": [
        "#### Data Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOVtMvIySM6G"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "#from konlpy.tag import Hannanum\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vndq9igzSFxu"
      },
      "source": [
        "import os\n",
        "data_root = \"/content/drive/MyDrive/공모전/BOAZ_KED 공모전/KEDxBOAZ/\"\n",
        "traindata_route = os.path.join(data_root, 'train_dfC.csv')\n",
        "traindata = pd.read_csv(traindata_route, index_col = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9CPa7o3-Yml"
      },
      "source": [
        "### Data tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1BTqdrgUHaT"
      },
      "source": [
        "def data_tokenizing(data, length, code, all = True):\n",
        "    \"\"\"\n",
        "    data -> dataframe의 형태\n",
        "    length -> int\n",
        "    length 매개변수는 최대 사용할 문자열의 길이의 설정을 위해서이다.\n",
        "    code 매개변수는 대/중/소/세/세세 분류 중에 어떤 것을 예측할지 설정해 주는 용도이다.\n",
        "    \"\"\"\n",
        "    CODE = {'대분류' : 1, '중분류' : 3, '소분류' : 4, '세분류' : 5, '대소분류':4}\n",
        "\n",
        "    from keras.preprocessing.text import Tokenizer\n",
        "    from keras.preprocessing.sequence import pad_sequences\n",
        "    \n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(data['BZ_PPOS_ITM_CTT'].astype(str))\n",
        "    \n",
        "    train_seq = tokenizer.texts_to_sequences(data['BZ_PPOS_ITM_CTT'].astype(str))\n",
        "    word_vocab = tokenizer.word_index\n",
        "    \n",
        "    MAX_SEQ_LENGTH = length # 사용할 문자열의 최대 길이\n",
        "    \n",
        "    train_inputs = pad_sequences(train_seq, maxlen = MAX_SEQ_LENGTH, padding = 'pre')\n",
        "    \n",
        "    #from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "    #encoder = LabelEncoder()\n",
        "    label_size = CODE[code]\n",
        "    if all == True:\n",
        "      train_labels = np.array(list(map(lambda x : str(x)[:label_size], data.index.values)))\n",
        "    else:\n",
        "      if code == '중분류':\n",
        "        train_labels = np.array(list(map(lambda x: str(x)[1:3], data.index.values)))\n",
        "      elif code == '소분류':\n",
        "        train_labels = np.array(list(map(lambda x: str(x)[3]), data.index.values))\n",
        "      elif code == '세분류': \n",
        "        train_labels = np.array(list(map(lambda x: str(x)[4], data.index.values)))\n",
        "      elif code == '대소분류':\n",
        "        train_labels = np.array(list(map(lambda x: str(x)[0] + str(x)[3] if len(x) > 4 else str(x)[0], data.index.values)))\n",
        "      # elif code == '소세세':\n",
        "      #   train_labels = np.array(list(map(lambda x: x[3:5] if len(x) > 4 else (''), data.index.values)))\n",
        "))\n",
        "    \n",
        "    # 데이터의 정보를 담고 있는 dictionary형의 자료\n",
        "    data_configs = {} \n",
        "    data_configs['vocab'], data_configs['vocab_size'] = word_vocab, len(word_vocab)+1\n",
        "    \n",
        "    \"\"\"\n",
        "    train_inputs -> 토큰화가 진행되고 padding 또한 진행된 데이터를 반환\n",
        "    -> 모델에 입력할 수 있는 형태\n",
        "    train_labels -> 수치형 데이터로 예측해야하는 업종 코드를 반환 (One-Hot Encoding된 형태로 반환 -> 모델에서 categorical_crossentropy로 학습 시킬 수 있도록)\n",
        "    data_configs -> 단어 사전과 단어의 총 개수를 dictionary의 형태로 입력된 데이터를 반환\n",
        "    \"\"\"\n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "    enc = OneHotEncoder(sparse = False)\n",
        "    train_labels = enc.fit_transform(train_labels.reshape(-1, 1))\n",
        "    data_configs['raw_labels'] = enc.categories_\n",
        "    \n",
        "    return train_inputs, train_labels, data_configs, enc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6Gr5TGy-rjd"
      },
      "source": [
        "all = False # 대분류 시 all=True 로 변경\n",
        "category = '중분류'\n",
        "train_inputs, train_labels, data_configs, label_encoder = data_tokenizing(traindata, 150, category, all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_T8yvnI0N5n"
      },
      "source": [
        "### MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmpz-zo2kB0Q"
      },
      "source": [
        "#### Self Attention\n",
        "1. positional encoding은 embedding vector에 단어의 문장에서의 위치에 대한 정보를 제공하기 위해서 주어진다.\n",
        "2. positional encoding을 추가한 이후에는 의미의 similarity와 문장에서의 위치가 반영이 가능하게 될 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXuV4jPGciDq"
      },
      "source": [
        "#### Embedding\n",
        "1. 먼저 positional encoding vector을 만들어야 한다.\n",
        "  - 문장 내에서 단어의 상대적인 의미를 제공해 줄 것이다.\n",
        "\n",
        "#### POSITIONAL ENCODING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ1uwhfIdM-P"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1/np.power(10000, (2*i//np.float32(d_model)))\n",
        "  return pos * angle_rates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0ozZj6ejiyu"
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :], d_model)\n",
        "  \n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype = tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mPRjSoiKV5L"
      },
      "source": [
        "#### MASK PADDING\n",
        "- 문자열의 모든 batch들에 대해서 padding된 token을 masking한다.\n",
        "- model이 padding된 부분을 **input으로 고려할 수 없도록**한다.\n",
        "- mask에 의해서 pad value가 존재하면 0, 존재하지 않으면 1로 설정이 되도록 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_6rZDfBKcld"
      },
      "source": [
        "def create_padding_masks(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "  # add extra dimensions to add the padding to the attention logits\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :] # (batch_size, 1, 1, seq_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rmIX8XfH57d"
      },
      "source": [
        "#### SCALED DOT PRODUCT ATTENTION\n",
        "-  Attention(Q, K, V) = softmax(Q*transpose(K) / d_model의 제곱근) x V"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96cW0uu0H98Z"
      },
      "source": [
        "def scaled_dot_product_attention(q, v, k, mask):\n",
        "  \"\"\"\n",
        "  @param q : query (.., seq_length_q, depth(=dmodel))\n",
        "  @param v : value (.., seq_length_v, depth(=dmodel))\n",
        "  @param k : key (.., seq_length_k, depth_v)\n",
        "  @param mask : (.., seq_length_q, seq_length_k)\n",
        "  \"\"\"\n",
        "\n",
        "  qk_matmul = tf.matmul(q, k, transpose_b = True)\n",
        "\n",
        "  # dk = dimension of queries ans key\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_att_logits = qk_matmul / tf.math.sqrt(dk)\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled_att_logits += (mask * -1e9)\n",
        "  \n",
        "  att_weights = tf.nn.softmax(scaled_att_logits, axis = -1)\n",
        "\n",
        "  output = tf.matmul(att_weights, v)\n",
        "\n",
        "  return output, att_weights\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okFVklKWH9Px"
      },
      "source": [
        "#### MULTI HEAD ATTENTION\n",
        "- Linear Layers and Splits into Heads\n",
        "- Scaled Dot-Product Attention\n",
        "- Concatenation of Heads\n",
        "- Final Linear Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4nKUgUjRmvq"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    '''\n",
        "    d_model : dimension을 알려주는, 즉 Attention을 연산할 때 key, query, value에 대해서 차원을 정의하기 위해 사용\n",
        "    num_heads : Attention의 head의 개수를 정의하기 위한 parameter\n",
        "    이 값들을 따로 kwargs에 저장할까 하다가 복잡해 질것 같아서 config.yaml 파일을 만들기로\n",
        "    '''\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "# 차원의 수는 head의 수만큼 나누어져야 하므로 나누어지지 않을 시에 Error발생\n",
        "    # assert d_model % num_heads == 0\n",
        "    \n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.depth = d_model\n",
        "\n",
        "    self.fcq = tf.keras.layers.Dense(d_model) # query의 차원만큼 Dense unit 설정\n",
        "    self.fcv = tf.keras.layers.Dense(d_model) # value\n",
        "    self.fck = tf.keras.layers.Dense(d_model) # key\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "  \n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "\n",
        "    q = self.fcq(q)\n",
        "    k = self.fck(k)\n",
        "    v = self.fcv(v)\n",
        "\n",
        "    q = self.split_heads(q, batch_size)\n",
        "    k = self.split_heads(k, batch_size)\n",
        "    v = self.split_heads(v, batch_size)\n",
        "\n",
        "    scaled_attention, attention_w = scaled_dot_product_attention(q,k,v,mask) # attention_w는 여기서는 필요 없는 값\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm = [0,2,1,3])\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "\n",
        "    output = self.dense(concat_attention)\n",
        "\n",
        "    return output, attention_w\n",
        "  \n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"\n",
        "    key, query, value의 값들에 대한 벡터를 각각의 head의 개수만큼 나눌 수 있도록 함\n",
        "    (batch_size, len_seq, depth) -> (batch_size, num_head, sequence, feature_size)\n",
        "    마지막 차원을 (num_heads, depth)로 변경해 줌\n",
        "    결과를 (batch_size, num_heads, seq_len, depth)로 transpose 해 줌\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    x = tf.transpose(x, perm = [0,2,1,3])\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdbpeqde1gAZ"
      },
      "source": [
        "#### ENCODER LAYER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPIY073MRgHP"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate = 0.1):\n",
        "    # positional encoding한 출력값에 적용한 dropout rate는 0.1 (논문에 의하면)\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
        "\n",
        "    self.do1 = tf.keras.layers.Dropout(rate)\n",
        "    self.do2 = tf.keras.layers.Dropout(rate)\n",
        "  \n",
        "  def call(self, x, training, mask):\n",
        "    att_output, _ = self.mha(x, x, x, mask = None) # (batch_size, input_seq_length, d_model)\n",
        "    att_output = self.do1(att_output, training) \n",
        "    out1 = self.layernorm1(x + att_output) \n",
        "\n",
        "    ffn_output = self.ffn(out1) # (batch_size, input_seq_length, d_model)\n",
        "    ffn_output = self.do2(ffn_output, training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output) # (batch_size, input_seq_length, d_model)\n",
        "\n",
        "    return out2 # (batch_size, input_seq_length, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O2cZe0eHLqW"
      },
      "source": [
        "#### POINT WISE FEED FORWARD NETWORK\n",
        "1. two fully-connected layers\n",
        "2. ReLU activation function을 두개의 완전 연결 층의 사이에 둔 network이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ISr8YmsHcLQ"
      },
      "source": [
        "def point_wise_feed_forward(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation = 'relu'), # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model) # (batch_size, seq_len. d_model)\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKNi8s4JxPMf"
      },
      "source": [
        "#### ENCODER\n",
        "1. Input Embedding\n",
        "2. Positional Encoding\n",
        "3. N encoder layers\n",
        "- 입력된 텍스트 데이터는 Embedding 층을 거쳐서 positional encoding과 결합이 된다.\n",
        "- 결합된 데이터는 encoder layer의 입력값으로 반환이 된다.\n",
        "- Encoder는 최종적으로 **word embedding과 position embedding 정보를 받아 input text에 대한 context information을 출력**한다.\n",
        "- 출력 데이터의 shape는 **(batch_size, input_seq_length, d_model)**이다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucnxYHcFHAa3"
      },
      "source": [
        "### Encoder class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSGOeAPrADb0"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_voca_size, \n",
        "               max_pos_encoding, rate = 0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    self.num_heads = num_heads\n",
        "    self.dff = dff # forward 층에 대해서 unit의 크기를 의미\n",
        "    self.embedding = tf.keras.layers.Embedding(input_voca_size, d_model, input_length = 150) # (batch_size, input_length, d_model)\n",
        "    self.pos_encoding = positional_encoding(max_pos_encoding, d_model)\n",
        "\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(self.num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.embedding(x) # tf.Tensor\n",
        "    # x = tf.Session().run(x) \n",
        "    # x = x.numpy()\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) # tf.Tensor\n",
        "    x += self.pos_encoding[:, :tf.shape(x)[1], :] # positional encoding의 경우에는 input embedding과 차원의 크기를 d_model로 동일하게 설정\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    #for layers in self.enc_layers:\n",
        "     # x = layers(x,mask = None)\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, mask = None)\n",
        "    \n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXCPjlhU1-Vp"
      },
      "source": [
        "### CLASSIFIER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znjQrNKCa_uR"
      },
      "source": [
        "class Classifier(tf.keras.Model):\n",
        "  def __init__(self, enc_configs, data_configs, **kwargs):\n",
        "    \"\"\"\n",
        "    @param encoder : 객체 입력\n",
        "    @param data_configs : data에 대한 정보가 dictionary의 형태로 저장\n",
        "    \"\"\"\n",
        "    super(Classifier, self).__init__()\n",
        "    # enc_configs['add_input']에 저장된 값이 None이 아닌 이상 입력으로 같이 주어져야 함\n",
        "    # 모델을 이런식으로 학습시켰다면 예측 또한 같은 방법으로 시켜야 함\n",
        "    \n",
        "\n",
        "    self.num_layers = enc_configs['num_layers']\n",
        "\n",
        "    self.added_input = enc_configs['add_input'] # pre-label 입력\n",
        "    self.type = enc_configs['type'] # \n",
        "    self.raw_labels = data_configs['raw_labels'][0]\n",
        "    self.encoder =  Encoder(num_layers = enc_configs['num_layers'], \n",
        "                            d_model = enc_configs['d_model'], num_heads = enc_configs['num_heads'], \n",
        "                    dff = enc_configs['dff'], max_pos_encoding = enc_configs['num_heads'], \n",
        "                    input_voca_size = data_configs['vocab_size'])\n",
        "    \"\"\"\n",
        "    출력 shape는 (batch_size, input_seq, d_model)\n",
        "    Dense Layer에 입력하기 위해서 Squeeze등의 과정은 불필요\n",
        "    \"\"\"\n",
        "    # shared_dense_args = {\n",
        "    #  \"units\" : , \"num_layers\" : ,\n",
        "    #   \"dropout\" : ,\n",
        "    # }\n",
        "    #self.flatten = tf.keras.layers.Flatten()\n",
        "    self.gru = tf.keras.layers.GRU(512, activation = 'relu')\n",
        "\n",
        "    self.main_layer = [\n",
        "      tf.keras.layers.Dense(units = 512, activation = 'relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dropout(0.3)\n",
        "    ]\n",
        "\n",
        "    self.fin = tf.keras.layers.Dense(len(self.raw_labels), activation = 'softmax')\n",
        "  \n",
        "  def return_logits(self, x):\n",
        "    raw_labels = data_configs['raw_labels'][0]\n",
        "    logits = []\n",
        "    x = self.encoder(x)\n",
        "    x = self.flatten(x)\n",
        "    for layer in main_layer:\n",
        "      x = layer(x)\n",
        "      for i in x:\n",
        "        answer_idx = tf.argmax(i, axis = -1)\n",
        "        logits.append(raw_labels[answer_idx])\n",
        "    \n",
        "    return logits\n",
        "\n",
        "  def merge_inputs(self, type, x):\n",
        "    if self.type != None:\n",
        "      add_input = self.added_input\n",
        "\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.encoder(x) # (batch_size, input_seq_len, d_model)\n",
        "    #x = self.flatten(x)\n",
        "    x = self.merge_inputs(self.type, x)\n",
        "\n",
        "    x = self.gru(x)\n",
        "    for _ in range(self.num_layers):\n",
        "      for layers in self.main_layer:\n",
        "        x = layers(x)\n",
        "    \n",
        "    result = self.fin(x)\n",
        "    #result = self.return_logits(result)\n",
        "    \n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8eOnJ8bJ_Bk"
      },
      "source": [
        "### HYPERPARAMETER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7hkhIHsKBNU"
      },
      "source": [
        "NUM_LAYERS = 4 #\n",
        "D_MODEL = 150 # embedding vector의 차원과 동일\n",
        "DFF = 512\n",
        "NUM_HEADS = 150 #\n",
        "INPUT_VOCA_SIZE = data_configs['vocab_size']\n",
        "BATCH_SIZE = 256\n",
        "EPOCH = 20\n",
        "\n",
        "enc_configs = {}\n",
        "\n",
        "enc_configs['num_layers'] = NUM_LAYERS\n",
        "enc_configs['d_model'] = D_MODEL\n",
        "enc_configs['num_heads'] = NUM_HEADS\n",
        "enc_configs['dff'] = DFF\n",
        "enc_configs['max_pos_encoding'] = 150 # max_pos_encoding\n",
        "enc_configs['add_input'] = None\n",
        "enc_configs['type'] = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zrlgiza8EoZe"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(train_inputs, train_labels, test_size = 0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQKBqyxsJ6iZ"
      },
      "source": [
        "1. Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsSUlvxsJ6RM"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 3e-4, beta_1 = 0.9, beta_2 = 0.98, epsilon = 10**(-9))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQw45hAcJ9uW"
      },
      "source": [
        "2. Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q9ZzNHREAkq"
      },
      "source": [
        "loss = tf.keras.losses.CategoricalCrossentropy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PivuGA3eJWJj"
      },
      "source": [
        "3. Training and Checkpointing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LRx0ftOgWDa"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "earlystopping = EarlyStopping(\n",
        "    monitor='val_loss', min_delta=0, patience=5, verbose=0,\n",
        "    mode='auto', baseline=None, restore_best_weights=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3FGfJP1GbSd"
      },
      "source": [
        "## Model train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aaRls0eOVWd"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  Model = Classifier(enc_configs = enc_configs, data_configs = data_configs)\n",
        "  Model.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])\n",
        "  history = Model.fit(x_train, y_train,  validation_split = 0.3, epochs = EPOCH,  batch_size = BATCH_SIZE, callbacks = earlystopping)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vdkuAl6-1u8"
      },
      "source": [
        "#### Train 결과 시각화 & evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wUlhjVhlpYp"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def merge_plot(history):\n",
        "  fig, loss_ax = plt.subplots()\n",
        "  acc_ax = loss_ax.twinx()\n",
        "\n",
        "  loss_ax.plot(history.history['loss'], 'y', label  = 'loss')\n",
        "  loss_ax.plot(history.history['val_loss'], 'r', label = 'val_loss')\n",
        "  loss_ax.set_xlabel('epoch')\n",
        "  loss_ax.set_ylabel('loss')\n",
        "  loss_ax.legend(loc = 'upper left')\n",
        "\n",
        "  acc_ax .plot(history.history['accuracy'], 'b', label = 'accuracy')\n",
        "  acc_ax.plot(history.history['val_accuracy'], 'g', label = 'val_accuracy')\n",
        "  acc_ax.set_ylabel('accuracy')\n",
        "  acc_ax.legend(loc = 'upper left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GArgK7jrZbbK"
      },
      "source": [
        "merge_plot(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNNxtRnvZTiT"
      },
      "source": [
        "Model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nT6hfuE85mz"
      },
      "source": [
        "### 예측기 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i28Q278w9fTp",
        "outputId": "8aaa46cf-9052-4f8f-c7f4-c83111ec148e"
      },
      "source": [
        "import os\n",
        "modelpath = \"/content/drive/MyDrive/공모전/BOAZ_KED 공모전/MODELS\"\n",
        "Model.save(os.path.join(modelpath, 'Model'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_30_layer_call_and_return_conditional_losses, embedding_30_layer_call_fn, dropout_319_layer_call_and_return_conditional_losses, dropout_319_layer_call_fn, embedding_30_layer_call_fn while saving (showing 5 of 210). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as embedding_30_layer_call_and_return_conditional_losses, embedding_30_layer_call_fn, dropout_319_layer_call_and_return_conditional_losses, dropout_319_layer_call_fn, embedding_30_layer_call_fn while saving (showing 5 of 210). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/KEDxBOAZMY/MODELS/Model_A/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/KEDxBOAZMY/MODELS/Model_A/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYdSA3b3k9fn",
        "outputId": "534921f0-d136-4088-f60e-bf88ee060486"
      },
      "source": [
        "Model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"classifier\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder (Encoder)            multiple                  164956148 \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  1019904   \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             multiple                  262656    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo multiple                  2048      \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             multiple                  40014     \n",
            "=================================================================\n",
            "Total params: 166,280,770\n",
            "Trainable params: 166,279,746\n",
            "Non-trainable params: 1,024\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bCeR5za5Gwr"
      },
      "source": [
        "### 4. Test Data에 대한 예측 도출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGj4uLqzJc9o"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "modelpath = \"/content/drive/MyDrive/공모전/BOAZ_KED 공모전/MODELS\"\n",
        "def load_model(model_name):\n",
        "  model_path = os.path.join(modelpath, str(model_name))\n",
        "  model_A = tf.keras.models.load_model(model_path)\n",
        "\n",
        "  return model_A"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QS5E1EHw5LQP",
        "outputId": "4fa5c3c4-23c1-450f-ac09-98bb3ede07ae"
      },
      "source": [
        "Model_T = load_model('Model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mql5WuLI2n-j"
      },
      "source": [
        "### Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO50ZW_IyMeT"
      },
      "source": [
        "def test(model, testdata, pad_len, encoder, category):\n",
        "    # plus : 필요없음\n",
        "    test_inputs, test_labels, testdata_configs, plus = data_tokenizing(testdata, pad_len, category)\n",
        "    y_pred = model.predict(test_inputs)\n",
        "    y_pred = encoder.inverse_transform(y_pred)\n",
        "\n",
        "    # 예측 결과 dataframe으로 반환\n",
        "    y_pred = pd.DataFrame(data=y_pred, index=testdata.index.values, columns=['모델이 예측한 업종코드'])\n",
        "\n",
        "    return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQv9XhYfr5Ol"
      },
      "source": [
        "def to_format(path, y_pred, testdata):\n",
        "    # path : 최종 결과 저장할 위치 \n",
        "    # y_pred : 예측 결과 dataframe\n",
        "    # testdata : testdata\n",
        "    data_root = \"/content/drive/MyDrive/공모전/BOAZ_KED 공모전/KEDxBOAZ\"\n",
        "    save_route = os.path.join(data_root, '[공모전]정답제출양식_(모동숙)_210523.xlsx')\n",
        "\n",
        "    format = pd.read_excel(path, index_col=0)\n",
        "\n",
        "    final_df = y_pred.reindex(format.index.values)\n",
        "    final_df = final_df.reset_index().rename(columns={\"index\": \"KEDCD\"})\n",
        "    final_df.to_excel(save_route, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PclaPT-wmpg0"
      },
      "source": [
        "TEST 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZlQ3jYhmkAl"
      },
      "source": [
        "import os\n",
        "data_root = \"/content/drive/MyDrive/공모전/BOAZ_KED 공모전/KEDxBOAZ\"\n",
        "testdata_route = os.path.join(data_root, 'test_df.csv')\n",
        "testdata = pd.read_csv(testdata_route, index_col = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8oax2acI2at"
      },
      "source": [
        "# label_encoder : train data tokenizing시 label encoder\n",
        "# category = '대분류'\n",
        "category = '중분류' \n",
        "# category = '소분류'\n",
        "# category = '세분류'\n",
        "# category = '대소분류'\n",
        "\n",
        "y_pred = test(Model_T, testdata, 150, label_encoder, category)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44Tf6kDkit1T"
      },
      "source": [
        "submission_path = os.path.join(data_root, '[공모전]정답제출양식_(조)_제출일자.xlsx')\n",
        "to_format(submission_path, y_pred, testdata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aGxld_rixko"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1Df5Ln2ixic"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXNWklyhixft"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh-mqha9ixc_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-GieceJST7W"
      },
      "source": [
        "----------------------"
      ]
    }
  ]
}